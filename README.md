# DMER
A survey of deep multimodal emotion recognition.

***
<br></br>
## Structure

- [Related Github Repositories](#related-github-repositories)
- [Datasets](#datasets)
- [Related Challenges](#related-challenges)
- [Related Projects](#related-projects)
- [Related Reviews](#related-reviews)
- [Related Papers](#related-papaers)

<br></br>

## Related Github Repositories
- [AWESOME-MER](https://github.com/EvelynFan/AWESOME-MER)
- [Reading list for Awesome Sentiment Analysis papers](https://github.com/declare-lab/awesome-sentiment-analysis)
- [AWESOME-MSA](https://github.com/thuiar/AWESOME-MSA)

<br></br>

## Datasets
| Datasets | Year | Features | Paper | Used |
| ----- | ----- | ----- | ----- | ----- |
| [EMOTIC Dataset](http://sunai.uoc.edu/emotic/) | 2019 TPAMI  | Face, Context | [Context based emotion recognition using emotic dataset](https://arxiv.org/pdf/2003.13401.pdf) |  | 
| [CMU-MOSEI](https://github.com/A2Zadeh/CMU-MultimodalSDK) | ACL 2018 | Visual, Audio, Language  | [Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph](https://www.aclweb.org/anthology/P18-1208.pdf) |  | 
| [ASCERTAIN Dataset](http://mhug.disi.unitn.it/wp-content/ASCERTAIN/ascertain.html) | 2018 TAC  | Facial activity data, Physiological data  | [ASCERTAIN: Emotion and Personality Recognition Using Commercial Sensors](https://ieeexplore.ieee.org/abstract/document/7736040) |  | 
| [RAVDESS](https://smartlaboratory.org/ravdess/) | 2018 PLoS ONE | Visual, Audio | [The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0196391) |  |
| [CMU-MOSI](http://multicomp.cs.cmu.edu/resources/cmu-mosi-dataset/) | 2016 IEEE Intelligent Systems | Visual, Audio, Language  | [Multimodal Sentiment Intensity Analysis in Videos: Facial Gestures and Verbal Messages](https://ww.sentic.net/multimodal-sentiment-intensity-analysis-in-videos.pdf) |  | 
| [Multimodal Spontaneous Emotion Database (BP4D+)](http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html) | CVPR 2016  | Face, Thermal data, Physiological data | [Multimodal Spontaneous Emotion Corpus for Human Behavior Analysis](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Multimodal_Spontaneous_Emotion_CVPR_2016_paper.pdf) |  | 
| [EmotiW Database](https://sites.google.com/view/emotiw2020) |    | Visual, Audio |  |  |
| [LIRIS-ACCEDE Database](https://liris-accede.ec-lyon.fr/index.php) |  2015 TAC  | Visual, Audio | [LIRIS-ACCEDE: A Video Database for Affective Content Analysis](https://liris.cnrs.fr/Documents/Liris-7059.pdf) |  |
| [CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D) |  2014 TAC  | Visual, Audio | [CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset](https://ieeexplore.ieee.org/document/6849440) |  |
| [SEMAINE Database](https://github.com/CheyneyComputerScience/CREMA-D) |  2014 TAC  | Visual, Audio | [CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset](https://ieeexplore.ieee.org/document/6849440) |  |
| [MAHNOB-HCI](https://ibug.doc.ic.ac.uk/resources/mahnob-hci-tagging-database/) |  2011 TAC  | Visual, Eye gaze, Physiological data | [A Multimodal Database for Affect Recognition and Implicit Tagging](https://ieeexplore.ieee.org/document/5975141) |  |
| [IEMOCAP Database](https://sail.usc.edu/iemocap/) |  2008 LRE  | Visual, Audio, Text transcripts | [IEMOCAP: Interactive emotional dyadic motion capture database](https://sail.usc.edu/publications/files/bussolre2008.pdf) |  |
| [eNTERFACE Dataset](http://enterface.net/) | ICDEW 2006 | Visual, Audio | [The eNTERFACE'05 audio-visual emotion database](http://poseidon.csd.auth.gr/papers/PUBLISHED/CONFERENCE/pdf/Martin06a.pdf) |  |
POM
MEmoR: A Dataset for Multimodal Emotion Reasoning in Videos

<br></br>

## Related Challenges
- [Multimodal (Audio, Facial and Gesture) based Emotion Recognition Challenge (MMER) @ FG](https://fg2020.org/competitions/competitions-cer-mmer/)
- [Emotion Recognition in the Wild Challenge (EmotiW) @ ICMI](https://sites.google.com/view/emotiw2020)
- [Audio/Visual Emotion Challenge (AVEC) @ ACM MM](https://sites.google.com/view/avec2019/home)
- [One-Minute Gradual-Emotion Behavior Challenge @ IJCNN](https://www2.informatik.uni-hamburg.de/wtm/OMG-EmotionChallenge/)
- [Multimodal Emotion Recognition Challenge (MEC) @ ACII](http://www.chineseldc.org/htdocsEn/emotion.html) 
- [Multimodal Pain Recognition (Face and Body) Challenge (EmoPain) @ FG](https://mvrjustid.github.io/EmoPainChallenge2020/)


<br></br>

## Related Projects
- [CMU Multimodal SDK](https://github.com/A2Zadeh/CMU-MultimodalSDK)
- [DELTA](https://github.com/Delta-ML/delta)
- [SenticNet](https://github.com/SenticNet)

<br></br>

## Related Reviews
- [Multimodal Intelligence: Representation Learning, Information Fusion, and Applications](https://arxiv.org/pdf/1911.03977.pdf)--(IEEE Journal of Selected Topics in Signal Processing, 2020)
- [A snapshot research and implementation of multimodal information fusion for data-driven emotion recognition](https://www.sciencedirect.com/science/article/pii/S1566253519301381)--(Information Fusion, 2020)
- [Survey on AI-Based Multimodal Methods for Emotion Detection](https://link.springer.com/content/pdf/10.1007%2F978-3-030-16272-6_11.pdf)--(High-Performance Modelling and Simulation for Big Data Applications, 2019)
- [Multimodal machine learning: A survey and taxonomy](https://arxiv.org/pdf/1705.09406.pdf)--(IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018)
- [A review of affective computing: From unimodal analysis to multimodal fusion](https://ww.sentic.net/affective-computing-review.pdf)--(Information Fusion, 2017)
- [A survey of multimodal sentiment analysis](https://ibug.doc.ic.ac.uk/media/uploads/documents/multi_modal.pdf)--(Image and Vision Computing, 2017)
- [A Review and Meta-Analysis of Multimodal Affect Detection Systems](https://dl.acm.org/doi/10.1145/2682899)--(ACM Computing Surveys, 2015)


<br></br>

### Video-Audio Method
| Index | Model | Paper | Year | Project | Dataset | Method |
| ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| VA-0 | VAANet | [An End-to-End Visual-Audio Attention Network for Emotion Recognition in User-Generated Videos](https://arxiv.org/pdf/2003.00832.pdf) | AAAI 2020  | [[coding](https://github.com/maysonny/VAANet)]  | VideoEmotion-8, Ekman-6 | Visual+Audio, Attention-Based Model|
| VA-1 |  | [Audio-Visual Emotion Forecasting: Characterizing and Predicting Future Emotion Using Deep Learning](https://www.academia.edu/39068756/Audio_visual_emotion_FG) | FG 2019 |  [[coding](https://github.com/sadat1971/Emotion-Forecasting-MATLAB_python)] |  IEMOCAP |  Face + Speech, Emotion forecasting|
| VA-2 | MMDDN | [Multimodal Deep Denoise Framework for Affective Video Content Analysi](https://dl.acm.org/doi/pdf/10.1145/3343031.3350997?casa_token=gd2Jxk5TbjgAAAAA:gqeFkRYLctpnYgiZqYoi19NWAjmoTFWoFN1U6TPJZmhY6hXB2KRA7-pS_907PD4pY2d0sD16sj7Ktw) | MM 2019 |   | LIRIS-ACCEDE |  Visual(colors, facial expressions, human gestures) + Audio(pitch, tone, and background music)|
| VA-3 | DBN | [Deep learning for robust feature generation in audiovisual emotion recognition](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.428.5585&rep=rep1&type=pdf) | ICASSP 2019 |   | IEMOCAP  |  Visual + Audio|
| VA-4 | MAFN | [Multi-Attention Fusion Network for Video-based Emotion Recognition](https://dl.acm.org/doi/pdf/10.1145/3340555.3355720?casa_token=jLoBxFwpv8oAAAAA:PkFQuCfQSR_YQ2MgV90YbtM7FXoIBirS7WK4OQgl7G97DM6DWWaDV_uuvY8VHW9abESvxzkNT5vkdA) | ICMI 2019 |   | AFEW |  Visual+Audio, Multiple attention fusion network|
| VA-5 | MERML | [Metric Learning-Based Multimodal Audio-Visual Emotion Recognition](https://project.dke.maastrichtuniversity.nl/RAI/wp-content/uploads/2019/12/Revised_MERML_IEEE_Multimedia_second_revisions.pdf) | 2019 TMM |   | eNTERFACE, CREMA-D |  Visual+Audio, Video and Audio Feature Extraction and Aggregation, MERML |
| VA-6 |  | [Audio-Visual Emotion Recognition in Video Clips](https://www.researchgate.net/publication/317487334_Audio-Visual_Emotion_Recognition_in_Video_Clips) | 2019 TAC |   | SAVEE, NTERFACE’05, RML |  Visual + Audio|
| VA-7 | EmoBed | [EmoBed: Strengthening Monomodal Emotion Recognition via Training with Crossmodal Emotion Embeddings](https://arxiv.org/pdf/1907.10428.pdf) | 2019 TAC |   | s RECOLA, OMG-Emotion |  Face + Audio|
| VA-8 |  | [Affective video content analysis based on multimodal data fusion in heterogeneous networks](https://www.sciencedirect.com/science/article/abs/pii/S1566253518307309) | 2019 Information Fusion |   |  |  Visual + Audio|
| VA-9 |  | [Audio-visual emotion fusion (AVEF): A deep efficient weighted approach](http://www.ece.ubc.ca/~minchen/min_paper/2019/2019-InformationFusion-3-AVEF.pdf) | 2019 Information Fusion |   | RML, Enterface05, BAUM-1s |  Visual + Audio|
| VA-10 |  | [Joint low rank embedded multiple features learning for audio–visual emotion recognition](https://www.sciencedirect.com/science/article/abs/pii/S092523122030045X) | 2019 Neurocomputing |   |  |  Visual + Audio|
| VA-11 | MLRF | [Multimodal Local-Global Ranking Fusion for Emotion Recognition](https://dl.acm.org/doi/pdf/10.1145/3242969.3243019?casa_token=-gjakVcf2UcAAAAA:3eVFHTZ5lyDglZLTy2YcV0iUDx5t99VI4NhxBt_b-zMkhHbqOnf4sbtyyCDKdhrbwaN_p-bnmrqcHQ) | ICMI 2018 |   | AVEC16(RECOLA) |  Visual + Audio|
| VA-12 |  | [A Combined Rule-Based & Machine Learning Audio-Visual Emotion Recognition Approach](https://ieeexplore.ieee.org/abstract/document/7506248/) | 2018 TAC |   | Cohn-Kanade, TheENTERFACE’05 |  Visual + Audio|
| VA-13 | MMDRBN | [A Multimodal Deep Regression Bayesian Network for Affective Video Content Analyses](https://openaccess.thecvf.com/content_ICCV_2017/papers/Gan_A_Multimodal_Deep_ICCV_2017_paper.pdf) | ICCV 2017  |  | LIRIS-ACCEDE | Video + Audio, Multimodal deep regression Bayesian network|
| VA-14 | ASER | [Automatic speech emotion recognition using recurrent neural networks with local attention](https://www.researchgate.net/profile/Seyedmahdad-Mirsamadi/publication/314756323_Automatic_Speech_Emotion_Recognition_Using_Recurrent_Neural_Networks_with_Local_Attention/links/59d9e6ddaca272e6096bc213/Automatic-Speech-Emotion-Recognition-Using-Recurrent-Neural-Networks-with-Local-Attention.pdf) | ICASSP 2017  | [[coding](https://github.com/gogyzzz/localatt_emorecog)] | IEMOCAP | Speech|
| VA-15 |  | [Modeling Multimodal Cues in a Deep Learning-Based Framework for Emotion Recognition in the Wild](https://dl.acm.org/doi/pdf/10.1145/3136755.3143006?casa_token=2A5_1a1ww4UAAAAA:w6MfCVPKhgCtOr1xXQL05mjNfPzQ5YS7ejhw15sXq9JAF99Et1JDvAVbIxtGuZoZUt-Bak1lbsYz7Q) | ICMI 2017 |   | AFEW, FER-2013, eNTERFACE dataset |  Visual + Audio|
| VA-16 |  | [Emotion recognition with multimodal features and temporal models](https://dl.acm.org/doi/pdf/10.1145/3136755.3143016?casa_token=03cMoedsLGkAAAAA:_Zul_lDV9yVGkQ1ulblBTUA9DAZqo797Ap9Xz0GJY6zUYLq2q3YYY6WUJj8eN_pll2w6-3c8zYXkVA) | ICMI 2017 |   | EmotiW(2017) |  Visual+Audio, Temporal model(LSTM) for facial features|
| VA-17 |  | [Learning Affective Features With a Hybrid Deep Model for Audio–Visual Emotion Recognition](http://www.jdl.link/doc/2011/201919_T-CSVT%20ID-2283.pdf) | 2017 T-CSVT |  [[coding](https://github.com/tzczsq/Learning-Affective-Features-with-a-Hybrid-Deep-Model-for-Audio-Visual-Emotion-Recognition)] | RML dataset,  eNTERFACE05, BAUM-1s |  Visual(3DCNN) +(DBN) Audio(CNN)|
| VA-18 | EEMER | [End-to-End Multimodal Emotion Recognition Using Deep Neural Networks](https://arxiv.org/pdf/1704.08619.pdf) | 2017 IEEE Journal of Selected Topics in Signal Processing  | [[coding](https://github.com/tzirakis/Multimodal-Emotion-Recognition)] | RECOLA |  Visual + Audio |
| VA-19 | MCEF | [Multi-cue fusion for emotion recognition in the wild](https://dl.acm.org/doi/pdf/10.1145/2993148.2997630?casa_token=MBly8gAf47IAAAAA:ZbuB6caI6H_ccSUmKJMaFsLEiQTwiuQOgVr1OgYaw8t3Yb90qrHAy3NjcU_vkEi62l5ksGpJst5nVg) | ICMI 2016 |   | cohn-kanade+, enterface’05  |  Visual + Audio|
| VA-20 | DemoFBVP | [ Multimodal emotion recognition using deep learning architectures](http://bcmi.sjtu.edu.cn/~blu/papers/2016/11.pdf) | WACV 2016 |   | [emoFBVP] |  Visual + Audio|
| VA-21 |  | [Exploring Cross-Modality Affective Reactions for Audiovisual Emotion Recognition](https://ieeexplore.ieee.org/abstract/document/6507534) | 2013 TAC |   | IEMOCAP, SEMAINE   |  Face + Audio|
| VA-22 |  | [Kernel Cross-Modal Factor Analysis for Information Fusion With Application to Bimodal Emotion Recognition](https://d1wqtxts1xzle7.cloudfront.net/46316929/2012tmm_ywang.pdf?1465324272=&response-content-disposition=inline%3B+filename%3DKernel_Cross_Modal_Factor_Analysis_for_I.pdf&Expires=1614825650&Signature=MkuZ5KKlGoDbn2ay81HV-LB5BzA9Y485i5NHLeJnZ8DNa2A15gB1LYsF4MMplMTQplqL-7Xcc7BFE5dIsZdVTC~1R1vWSo4~2XyESxcQzjbpm8WZbLCh722cTcfKum3Yr~0i8S3iDu4UY5zH4hNlWYSTyFYVBs4dVrc~lDvajro2X39rPtYA8~GsRWHnHRtTlGWw-p3mnTE4I7HQB6VI4hVto2DyPvdm00JJPsso30XfTk8NmjmIrHwkZTc4WILjCLgmtKIJS-aQ3INnbyxpJHvG221UJEvJl9WJxpDk~OPTAnmWJXZ-l4SKbSYFeJYA~Kn2PAP6kn6um4ZHF2f0cw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA) | 2012 TMM |   |  RML, eNTERFACE |  Visual + Audio, Kernel cross-modal factor analysis|




<br></br>

### Context-awarded method
| Index | Model | Paper | Year | Project | Dataset | Method |
| ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| CA-1 | EmotiCon | [EmotiCon: Context-Aware Multimodal Emotion Recognition using Frege’s Principle](https://openaccess.thecvf.com/content_CVPR_2020/papers/Mittal_EmotiCon_Context-Aware_Multimodal_Emotion_Recognition_Using_Freges_Principle_CVPR_2020_paper.pdf) | CVPR 2020  | [[video](https://www.youtube.com/watch?v=kYOFkL7n0AI)] [[project](https://gamma.umd.edu/researchdirections/affectivecomputing/emoticon/)] | EMOTIC, [GroupWalk] | Face+Gait+(Depth+Background), Multiplicative fusion, etc|
| CA-2 | CAER-Net | [Context-Aware Emotion Recognition Networks](https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Context-Aware_Emotion_Recognition_Networks_ICCV_2019_paper.pdf) | ICCV 2019  | [[coding](https://github.com/ndkhanh360/CAER)][[project](https://caer-dataset.github.io/)] | EMOTIC, AffectNet, [CAER-S], AFEW, [CAER] | Face + Context, Adaptive Fusion|
| CA-3 |  | [Context-aware affective graph reasoning for emotion recognition](https://ieeexplore.ieee.org/abstract/document/8784981) | ICME 2019 |   |   |  |
| CA-4 |  | [Context Based Emotion Recognition using EMOTIC Dataset](https://arxiv.org/pdf/2003.13401.pdf) | 2019 TPAMI | [[coding](https://github.com/rkosti/emotic)]  | EMOTIC |  Face + Context|
| CA-5 |  | [Multimodal Framework for Analyzing the Affect of a Group of People](http://jultika.oulu.fi/files/nbnfi-fe2019040511211.pdf) | 2018 TMM |   | HAPPEI, GAFF  |  Face+Upper body+Scene, Face-based Group-level Emotion Recognition|
| CA-6 |  | [Emotion Recognition in Context](https://openaccess.thecvf.com/content_cvpr_2017/papers/Kosti_Emotion_Recognition_in_CVPR_2017_paper.pdf) | CVPR 2017  | [[project](http://sunai.uoc.edu/emotic/)] | [EMOTIC] | Body feature+Image feature(Context)|

<br></br>

### Video-Audio-Text method
| Index | Model | Paper | Year | Project | Dataset | Method |
| ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| VAT-1 | Self-MM | [Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis](https://arxiv.org/pdf/2102.04830.pdf) | AAAI 2021  | [[coding](https://github.com/thuiar/Self-MM)]   | CMU-MOSI, CMU-MOSEI, SIMS | Video+Speech+Text|
| VAT-2 | CTNet | [CTNet: Conversational Transformer Network for Emotion Recognition](https://ieeexplore.ieee.org/abstract/document/9316758/) | 2021 TASLP | - | IEMOCAP, MELD | Face+Speech+Text|
| VAT-3 | M3ER | [M3ER: Multiplicative Multimodal Emotion Recognition Using Facial, Textual, and Speech Cues](https://arxiv.org/pdf/1911.05659.pdf) | AAAI 2020  | [[video](https://www.youtube.com/watch?v=GEklunW5S6U&feature=emb_logo)] [[project](https://gamma.umd.edu/researchdirections/affectivecomputing/m3er/)]  | IEMOCAP, CMU-MOSEI | Face+Speech+Text, Multiplicative Fusion, CCA, Modality Check|
| VAT-4 | ARGF | [Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion](https://ojs.aaai.org/index.php/AAAI/article/view/5347) | AAAI 2020  |   | IEMOCAP, CMU-MOSEI, CMU-MOSI| Face+Speech+Text|
| VAT-5 | ICCN | [Learning Relationships between Text, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis](https://arxiv.org/pdf/1911.05544.pdf) | AAAI 2020  |   | IEMOCAP, CMU-MOSI, CMU-MOSEI | Video+Audio+Text|
| VAT-6 | MISA | [MISA: Modality-Invariant and-Specific Representations for Multimodal Sentiment Analysis](https://dl.acm.org/doi/abs/10.1145/3394171.3413678?casa_token=oI8VnZ8Eg10AAAAA:mVUbDA0AZiAXcDxiDmV9-ooRH4PxzlSMXkBCgm1OCopziDWz8U3ZU54VzJIfqCCsbRFAvk8_kJhzBQ) | ACM MM 2020  | [[coding](https://github.com/declare-lab/MISA)] | MOSI and MOSEI | Face+Speech+Text, |
| VAT-7 | CM-BERT | [CM-BERT: Cross-Modal BERT for Text-Audio Sentiment Analysis](https://dl.acm.org/doi/pdf/10.1145/3394171.3413690?casa_token=IbjKGuXqhgYAAAAA:UDOJDylxkH0cSNH4zpKslmJJIPavnD5krQbRyUiSetChiazXntBGTtMzGJvI1w4011XIzTgFPMVeMw) | ACM MM 2020  | [[coding](https://github.com/thuiar/Cross-Modal-BERT)] | MOSI and MOSEI | Speech+Text |
| VAT-8 | SWAFN | [SWAFN: Sentimental Words Aware Fusion Network for Multimodal Sentiment Analysis](https://www.aclweb.org/anthology/2020.coling-main.93.pdf) | ACL 2020  | [[coding1](https://github.com/GDUFS-NLP/SWAFN)] [[coding2](https://github.com/gdufsnlp/SWAFN)]  |  CMU-MOSI, CMU-MOSEI and YouTube | Video+Speech+Text |
| VAT-9 | MAG- | [Integrating Multimodal Information in Large Pretrained Transformers](https://www.aclweb.org/anthology/2020.acl-main.214/) | ACL 2020  | [[coding](https://github.com/WasifurRahman/BERT_multimodal_transformer)] |  CMU-MOSI, CMU-MOSEI | Video+Speech+Text |
| VAT-10 | MTEE | [Modality-Transferable Emotion Embeddings for Low-Resource Multimodal Emotion Recognition](https://www.aclweb.org/anthology/2020.aacl-main.30.pdf) | AACL-IJCNLP 2020 |  [[coding](https://github.com/wenliangdai/Modality-Transferable-MER)] | IEMOCAP, CMU-MOSEI |  Visual + Text + Audion|
| VAT-11 | Multimodal Routing | [Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis](https://www.aclweb.org/anthology/2020.emnlp-main.143.pdf) | EMNLP 2020  | [[coding](hhttps://github.com/martinmamql/multimodal_routing)] | IEMOCAP, CMU-MOSEI | Face+Speech+Text|
| VAT-12 | SF-SSL | [Jointly Fine-Tuning "BERT-like" Self Supervised Models to Improve Multimodal Speech Emotion Recognition](https://arxiv.org/pdf/2008.06682.pdf) | InterSpeech 2020  | [[coding](https://github.com/shamanez/BERT-like-is-All-You-Need)]   | IEMOCAP, CMU-MOSI, CMU-MOSEI | Video+Speech+Text|
| VAT-13 |  | [Multimodal Deep Learning Framework for Mental Disorder Recognition](https://www.cl.cam.ac.uk/~mmam3/pub/FG2020_Multimodal_Deep_Learning_Framework_for_Mental_Disorder_Recognition.pdf) | FG 2020 | [[coding](https://github.com/ZihengZZH/bipolar-disorder)]  | Bipolar Disorder Corpus (BDC)， Extended Distress Analysis Interview Corpus (E-DAIC) |  Visual + Audio + Text|
| VAT-14 | Deep-HOSeq | [Deep Higher Order Sequence Fusion for Multimodal Sentiment Analysis](https://arxiv.org/pdf/2010.08218.pdf) | ICDM  2020 | [[coding](https://github.com/sverma88/Deep-HOSeq--ICDM-2020)]  | CMU-MOSEI and CMU-MOSI  |  Visual + Audio + Text|
| VAT-15 | DFF-ATMF | [Complementary fusion of multi-features and multi-modalities in sentiment analysis](https://arxiv.org/pdf/1904.08138.pdf) | AAAI-WK 2020 | [[coding](https://github.com/sverma88/Deep-HOSeq--ICDM-2020)]  | CMU-MOSEI and CMU-MOSI, IEMOCAP |  Visual + Audio + Text|
| VAT-16 | MSAF | [MSAF: Multimodal Split Attention Fusion](https://arxiv.org/pdf/2012.07175.pdf) | 2020 | [[coding](https://github.com/anita-hu/MSAF)]  | RAVDESS, CMU-MOSEI, NTU RGB+D |  Visual + Audio + Text|
| VAT-17 | DCVDN | [Visual-Texual Emotion Analysis With Deep Coupled Video and Danmu Neural Networks](https://arxiv.org/pdf/1811.07485.pdf) | 2020 TMM |   | ISEAR, Multi-Domain Sentiment DataseT, [Video-Danmu] |  Visual + Text|
| VAT-18 | LMFN | [Locally Confined Modality Fusion Network With a Global Perspective for Multimodal Human Affective Computing](https://ieeexplore.ieee.org/abstract/document/8752006) | 2020 TMM |   | CMU-MOSEI, MELD, IEMOCAP |  Visual + Audio + Language|
| VAT-19 | SSE-FT | [Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206016) | 2020 IEEE ACCESS | [[coding](https://github.com/shamanez/Self-Supervised-Embedding-Fusion-Transformer)]  | CMU-MOSEI, IEMOCAP |  Visual + Audio + Language|
| VAT-20 | HPFN | [Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling](https://static.aminer.cn/misc/pdf/9381-deep.pdf) | NeurIPS 2019 | [[coding](https://github.com/jiajiatang0000/HPFN)]  | CMU-MOSI, IEMOCAP | Visual+Audio+Text|
| VAT-21 | MFM | [Learning Factorized Multimodal Representations](https://arxiv.org/pdf/1806.06176.pdf) | ICLR 2019 | [[coding](https://github.com/pliang279/factorized/)]  | CMU-MOSI, IEMOCAP, POM, MOUD, ICT-MMMO, and YouTube | Visual+Audio+Text|
| VAT-22 | MCTN | [Found in translation: Learning robust joint representations by cyclic translations between modalities](https://ojs.aaai.org/index.php/AAAI/article/view/4666) | AAAI 2019 | [[coding](https://github.com/hainow/MCTN)]  | CMU-MOSI, ICTMMMO, and YouTube | Visual+Audio+Text|
| VAT-23 | RAVEN | [Words Can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors](https://arxiv.org/pdf/1811.09362.pdf) | AAAI 2019 | [[coding](https://github.com/victorywys/RAVEN)]  | CMU-MOSI,IEMOCAP | Visual+Audio+Text|
| VAT-24 | HFFN | [Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing](https://www.aclweb.org/anthology/P19-1046.pdf) | ACL 2019 |  [[coding](https://github.com/TmacMai/multimodal-fusion)] | CMU-MOSEI, IEMOCAP |  Visual + Audio + Language|
| VAT-25 | DeepCU | [DeepCU: Integrating both Common and Unique Latent Information for Multimodal Sentiment Analysis](https://www.ijcai.org/Proceedings/2019/0503.pdf) | IJCAI 2019 | [[coding](https://github.com/sverma88/DeepCU-IJCAI19)]  | CMU-MOSI, POM | Visual+Audio+Text|
| VAT-26 | MMResLSTM | [Mutual Correlation Attentive Factors in Dyadic Fusion Networks for Speech Emotion Recognition](https://dl.acm.org/doi/pdf/10.1145/3343031.3351039?casa_token=Vast9_rsesgAAAAA:9xSY3zkcbJv6R1e8hP1fZsmkzvFPv0gi_7mtJhiDYlWfQCJx1wjnxGb7-QPsOeLaRa2hHJYog8DenQ) | MM 2019 | [[coding](https://github.com/XinyuLyu/Mutual-Correlation-Attentive-Factors-in-Dyadic-Fusion-Networks-for-Speech-Emotion-Recognition)] | IEMOCAP, MELD |  Audio+Text, Multimodal residual LSTM |
| VAT-27 | CIA | [Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis](https://www.aclweb.org/anthology/D19-1566.pdf) | EMNLP 2019 |  | MOUD, MOSI, YouTube, ICT-MMMO, and MOSEI |  Video+Audio+Text, Use a recurrent neural network to learn the inter-modal interaction among modalities with an auto-encoder. |
| VAT-28 | CIM-MTL | [Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis](https://arxiv.org/pdf/1905.05812.pdf) | NAACL 2019 |  | MOSEI |  Video+Audio+Text|
| VAT-29 | LAMER | [Learning Alignment for Multimodal Emotion Recognition from Speech](https://arxiv.org/pdf/1909.05645.pdf) | InterSpeech 2019 | [[coding](https://github.com/Delta-ML/delta)]  |  IEMOCAP |  Speech + Text|
| VAT-30 | MHA | [Speech emotion recognition using multi-hop attention mechanism](https://arxiv.org/pdf/1904.10788.pdf) | ICASSP 2019 | [[coding](https://github.com/Delta-ML/delta)]  |  IEMOCAP |  Audio + Text|
| VAT-31 | MSER | [Multimodal speech emotion recognition and ambiguity resolution](https://arxiv.org/pdf/1904.06022.pdf) | 2019 | [[coding](https://github.com/Demfier/multimodal-speech-emotion-recognition)]  |  IEMOCAP |  Audio + Text|
| VAT-32 | EmoBed | [EmoBed: Strengthening Monomodal Emotion Recognition via Training with Crossmodal Emotion Embeddings](https://arxiv.org/pdf/1907.10428.pdf) | 2019 TAC |   |  RECOLA, OMG-Emotion |  Visual + Text|
| VAT-33 | MulT | [Multimodal Transformer for Unaligned Multimodal Language Sequences](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7195022/) | 2019 NIH Public Access | [[coding](https://github.com/yaohungt/Multimodal-Transformer)]  |  CMU-MOSI & MOSEI, IEMOCAP |  Visual + Text + Speech|
| VAT-34 | MFN | [Memory fusion network for multi-view sequential learning](https://ojs.aaai.org/index.php/AAAI/article/view/12021) | AAAI 2018 | [[coding](https://github.com/pliang279/MFN)]  |  CMU-MOSI, ICT-MMMO,YouTube, MOUD, IEMOCAP and POM |  Visual + Text + Speech|
| VAT-35 | MARN | [Multi-Attention Recurrent Network for Human Communication Comprehension](https://ojs.aaai.org/index.php/AAAI/article/view/12024) | AAAI 2018 | [[coding](https://github.com/A2Zadeh/MARN)]  |  CMU-MOSI, ICT-MMMO,YouTube, MOUD, IEMOCAP and POM |  Visual + Text + Speech|
| VAT-36 | Graph-MFN | [Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph](https://www.aclweb.org/anthology/P18-1208.pdf) | ACL 2018 | [[coding](https://github.com/Delta-ML/delta)]  |  [[CMU-MOSEI](https://github.com/A2Zadeh/CMU-MultimodalSDK)]  |  Visual + Audio + Text|
| VAT-37 |LMF | [Efficient Low-rank Multimodal Fusion with Modality-Specific Factors](https://arxiv.org/pdf/1806.00064.pdf) | ACL 2018 | [[coding](https://github.com/Justin1904/Low-rank-Multimodal-Fusion)]  |  IEMOCAP, CMU-MOSEI, POM  |  Visual + Audio + Text, Perform multimodal fusion using low-rank tensors|
| VAT-38 | RMFN | [Multimodal Language Analysis with Recurrent Multistage Fusion](https://arxiv.org/pdf/1808.03920.pdf) | EMNLP 2018 |  |  IEMOCAP, CMU-MOSI, POM  |  Visual + Audio + Text|
| VAT-39 | MRTN | [Multimodal Relational Tensor Network for Sentiment and Emotion Classification](https://arxiv.org/pdf/1806.02923.pdf) | ACL-Challenge-HML 2018 | [[coding](https://github.com/Delta-ML/delta)]  |  CMUMOSEI  |  Visual + Audio + Text|
| VAT-40  | | [Convolutional attention networks for multimodal emotion recognition from speech and text data](http://multicomp.cs.cmu.edu/wp-content/uploads/2018/06/book.pdf#page=38) | ACL-Challenge-HML 2018 |   | CMU-MOSEI   |  Speech + Text|
| VAT-41 | HFusion | [Multimodal sentiment analysis using hierarchical fusion with context modeling](https://arxiv.org/pdf/1806.06228.pdf) | 2018 Knowledge-Based Systems  | [[coding](https://github.com/SenticNet/hfusion)]  | MOSI, IEMOCAP |  Visual + Audio + Text|
| VAT-42 | BC-LSTM | [Context-dependent sentiment analysis in user-generated videos](https://www.aclweb.org/anthology/P17-1081.pdf) | ACL 2017 |  [[coding](https://github.com/soujanyaporia/multimodal-sentiment-analysis/tree/f974f88ec973867b9411966624d732ff6985f66d)] | CMU-MOSI, MOUD, IEMOCAP   | Audio+Video+Text|
| VAT-43 |TFN| [ Tensor Fusion Networks for multimodal sentiment analysis](https://arxiv.org/pdf/1707.07250.pdf) | EMNLP 2017 |  [[coding](https://github.com/Justin1904/TensorFusionNetworks)] | CMU-MOSI | Audio+Video+Text|
| VAT-44  | MLMA| [Multi-level multiple attentions for contextual multimodal sentiment analysis](https://www.sentic.net/multiple-attentions-for-multimodal-sentiment-analysis.pdf) | ICDM 2017 |  [[coding](https://github.com/soujanyaporia/multimodal-sentiment-analysis/tree/f974f88ec973867b9411966624d732ff6985f66d)] | CMU-MOSI   | Audio+Video+Text|
| VAT-45  | MV-SLTM | [Extending long short-term memory for multi-view structured learning](https://www.researchgate.net/profile/Roland-Goecke/publication/308191678_Extending_Long_Short-Term_Memory_for_Multi-View_Structured_Learning/links/5a0bf0a7aca2721a23fa3043/Extending-Long-Short-Term-Memory-for-Multi-View-Structured-Learning.pdf) | ECCV 2016 |  | MMDB | Audio+Video+Text|
| VAT-46 |  | [Fusing audio, visual and textual clues for sentiment analysis from multimodal content](https://ww.w.sentic.net/multimodal-sentiment-analysis.pdf) | 2016 Neurocomputing |   | YouTube dataset, SenticNet,  EmoSenticNet  |  Visual + Audio + Text|
| VAT-47 |  | [Temporal Bayesian Fusion for Affect Sensing: Combining Video, Audio, and Lexical Modalities](https://ieeexplore.ieee.org/abstract/document/6930787) | 2015 TC |   | SEMAINE |  Face + Audio + Lexical features, Temporal Bayesian fusion|
| VAT-48 |  | [Towards an intelligent framework for multimodal affective data analysis](https://dspace.stir.ac.uk/bitstream/1893/21310/1/Neural%20Networks%202014.pdf) | 2015 Neural Networks |   | eNTERFACE  |  Visual + Audio + Text|



<br></br>

### Attribute-based
| Index | Model | Paper | Year | Project | Dataset | Method |
| ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| AB-1 | MMDRBN | [Knowledge-Augmented Multimodal Deep Regression Bayesian Networks for Emotion Video Tagging](https://www.ecse.rpi.edu/~cvrl/Publication/pdf/Wang2021.pdf) | 2019 TMM |   | LIRIS-ACCEDE |  Visual + Audio + Attribute|
| AB-2 |  | [Recognizing Induced Emotions of Movie Audiences From Multimodal Information](http://eprints.bournemouth.ac.uk/31903/1/j9-Recognizing%20Induced%20Emotions%20of%20Movie%20Audiences%20From%20Multimodal%20Information.pdf) | 2019 TAC |   | LIRIS-ACCEDE |  Visual + Audio + Dialogue + Attribute|
| AB-3 |  | [Multimodal emotional state recognition using sequence-dependent deep hierarchical features](https://www.sciencedirect.com/science/article/pii/S0893608015001847) | 2015 Neural Networks  |   |  FABO |  Face + Upper-body|
| AB-4 |  | [Context-Sensitive Learning for Enhanced Audiovisual Emotion Classification](https://sail.usc.edu/publications/files/metallinou-acii2015.pdf) | 2012 TAC |   | IEMOCAP  |  Visual + Audio + Utterance|
| AB-5 |  | [Continuous Prediction of Spontaneous Affect from Multiple Cues and Modalities in Valence-Arousal Space](https://www.researchgate.net/profile/Mihalis-Nicolaou-2/publication/224226954_Continuous_Prediction_of_Spontaneous_Affect_from_Multiple_Cues_and_Modalities_in_Valence-Arousal_Space/links/540af7a00cf2f2b29a2cdc36/Continuous-Prediction-of-Spontaneous-Affect-from-Multiple-Cues-and-Modalities-in-Valence-Arousal-Space.pdf) | 2011 TAC |   | SAL-DB |  Face + Shoulder gesture + Audio|



<br></br>

### Aspect-based Network
| Index | Model | Paper | Year | Project | Dataset | Method |
| ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| ABN-1 | MIMN | [Multi-Interactive Memory Network for Aspect Based Multimodal Sentiment Analysis](https://ojs.aaai.org//index.php/AAAI/article/view/3807) | AAAI 2019  | [[coding](https://github.com/xunan0812/MIMN)]  | [Multi-ZOL] | Text+Aspect+Images, Aspect based multimodal sentiment analysis|
| ABN-2 | VistaNet | [VistaNet: Visual Aspect Attention Network for Multimodal Sentiment Analysis](https://ojs.aaai.org//index.php/AAAI/article/view/3799) | AAAI 2019  | [[coding](https://github.com/PreferredAI/vista-net)]  | [Yelp-Food-Restaurants] | Visual+Text|
| ABN-3 |  | [Cooperative Multimodal Approach to Depression Detection in Twitter](https://ojs.aaai.org//index.php/AAAI/article/view/3775) | AAAI 2019  |   | Textual Depression Dataset, Multimodal Depression Dataset | Visual+Text, GRU+VGG-Net+COMMA|
| ABN-4 | TomBERT | [Adapting BERT for Target-Oriented Multimodal Sentiment Classification](https://www.ijcai.org/Proceedings/2019/0751.pdf) | IJCAI 2019 | [[coding](https://github.com/jefferyYu/TomBERT)]  | Multimodal Twitter datasets | Image+Text, BERT-based|
| ABN-5 |  | [Predicting Emotions in User-Generated Videos](https://ojs.aaai.org/index.php/AAAI/article/view/8724) | AAAI 2014 |   | [Dataset] | Visual+Audio+Attribute, Video content recognition|


<br></br>

### Physiological signal-based
| Index | Model | Paper | Year | Project | Dataset | Method |
| ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| PSB-1 |  | [Emotion Recognition From Multimodal Physiological Signals Using a Regularized Deep Fusion of Kernel Machine](https://www.researchgate.net/profile/Jian-Shen-32/publication/341361785_Emotion_Recognition_From_Multimodal_Physiological_Signals_Using_a_Regularized_Deep_Fusion_of_Kernel_Machine/links/5ebe55a2299bf1c09abc35ab/Emotion-Recognition-From-Multimodal-Physiological-Signals-Using-a-Regularized-Deep-Fusion-of-Kernel-Machine.pdf) | 2020 TC |   |  |  EEG + Other physiological signals|
| PSB-2 | MMResLSTM | [Emotion Recognition using Multimodal Residual LSTM Network](https://dl.acm.org/doi/pdf/10.1145/3343031.3350871?casa_token=eMp-HVIHiDEAAAAA:NYHcjx1W7rG_KA6vczPxlkWjgei1xQbmTfngt-0x7Gg54F2Pxh06nIaFxrz-usaoqFRY1ghSc-VUIg) | MM 2019 |   | DEAP |  EEG + PPS(EOG+EMG), Multimodal residual LSTM |
| PSB-3 |  | [EmotionMeter: A Multimodal Framework for Recognizing Human Emotions](http://bcmi.sjtu.edu.cn/~blu/papers/2019/1.pdf) | 2019 TC |   |  |  EEG + Eye movements|
| PSB-4 |   | [Personality-Aware Personalized Emotion Recognition from Physiological Signals](https://www.ijcai.org/Proceedings/2018/0230.pdf) | IJCAI 2018 |   | ASCERTAIN | Personality+Physiological signals, Personalized emotion recognition|
| PSB-5 |  | [Combining Facial Expression and Touch for Perceiving Emotional Valence](https://www.researchgate.net/profile/Jean-Claude-Martin/publication/310734026_Combining_Facial_Expression_and_Touch_for_Perceiving_Emotional_Valence/links/5bd7b1e592851c6b2798d364/Combining-Facial-Expression-and-Touch-for-Perceiving-Emotional-Valence.pdf) | 2018 TAC |   | KDEF |  Face + Touch stimuli|
| PSB-6 |  | [Multi-modality weakly labeled sentiment learning based on Explicit Emotion Signal for Chinese microblog](https://www.sciencedirect.com/science/article/abs/pii/S0925231217312298) | 2018 Neurocomputing |   |  |  Face + Touch stimuli|
| PSB-7 |  | [Analysis of EEG Signals and Facial Expressions for Continuous Emotion Detection](https://www.ibug.doc.ic.ac.uk/media/uploads/documents/journal_eegcontinuous.pdf) | 2016 TAC |   |  |  Face + EEG signals|
| PSB-8 |  | [Multi-modal emotion analysis from facial expressions and electroencephalogram](https://www.sciencedirect.com/science/article/abs/pii/S1077314215002106) | 2016 Computer Vision and Image Understanding  |   |  |  Face + EEG |
| PSB-9 |  | [Combining Eye Movements and EEG to Enhance Emotion Recognition](http://bcmi.sjtu.edu.cn/~zhengweilong/pdf/ijcai15_final_submittd.pdf) | IJCAI 2015 |   | [[dataset](http://bcmi.sjtu.edu.cn/∼seed/index.html)] |  Eye Movements+EEG signal|
| PSB-10 |  | [Multimodal Emotion Recognition in Response to Videos](https://d1wqtxts1xzle7.cloudfront.net/30736774/soleymani2011multimodal.pdf?1362142614=&response-content-disposition=inline%3B+filename%3DMultimodal_Emotion_Recognition_in_Respon.pdf&Expires=1614826781&Signature=OhtUCmuqf3zL7nMuMwgW6Po2eZFXIupKgjL61yYMGyRVHh90ZzD~TB29gDGGUSYnRuw1r-hRhmDXbZ39wxcavRxGg7E7RKe43CnOCMRUWbTAbVuKPipUOyWR-ZQdsq9hisFm2UiXXW8I3nPZ0cAPjcMY4lHG-FG~7dm18KPpD2RE2XEOsIIkL3YLLuq0WWQ~-EIQPX~TUoI-Yi4Xu5q~AkdMoHcXWPeKePlNelZHdIYCGBNhh0v9rc8Q1VtAmqQs5Kt3Dg585EBiUSkf-dcJR3QjrB6UyO8duUcTIUC5AClbgxOevlLShavo3OxW7MVTeJl4ukOBYyQ6NYOaZ2~8Xw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA) | 2012 TAC |   |  |  Eye gaze + EEG signals|

